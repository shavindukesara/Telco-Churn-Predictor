{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcY7C6I0Df8v4+Vk7RvyTt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shavindukesara/Telco-Churn-Predictor/blob/main/models/neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q scikit-learn tensorflow pandas numpy matplotlib seaborn\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, callbacks\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                            accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, roc_curve, auc)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Handle missing values\n",
        "df_processed['TotalCharges'] = pd.to_numeric(df_processed['TotalCharges'], errors='coerce')\n",
        "print(f\"Missing TotalCharges: {df_processed['TotalCharges'].isnull().sum()}\")\n",
        "\n",
        "# FIXED LINE: Use direct assignment instead of inplace=True\n",
        "df_processed['TotalCharges'] = df_processed['TotalCharges'].fillna(df_processed['TotalCharges'].median())\n",
        "\n",
        "# Drop non-predictive columns\n",
        "df_processed = df_processed.drop(['customerID', 'TenureGroup'], axis=1, errors='ignore')\n",
        "\n",
        "# Encode binary variables\n",
        "binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
        "for col in binary_cols:\n",
        "    df_processed[col] = df_processed[col].map({'Yes': 1, 'No': 0, 'Male': 1, 'Female': 0})\n",
        "\n",
        "# One-hot encode multi-class variables\n",
        "multi_class_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                    'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
        "                    'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n",
        "df_processed = pd.get_dummies(df_processed, columns=multi_class_cols, drop_first=True, dtype=int)\n",
        "\n",
        "# Encode target\n",
        "df_processed['Churn'] = df_processed['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Separate features and target\n",
        "X = df_processed.drop('Churn', axis=1)\n",
        "y = df_processed['Churn']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(\"Preprocessing complete\")\n",
        "\n",
        "\n",
        "# Calculate class weights\n",
        "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: class_weights_array[i] for i in range(len(class_weights_array))}\n",
        "print(f\"Class Weights: {class_weights}\")\n",
        "\n",
        "# Build model\n",
        "def create_nn_model(input_dim, learning_rate=0.001):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_dim=input_dim),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Hyperparameter tuning\n",
        "print(\"\\nHyperparameter Tuning...\")\n",
        "configs = [\n",
        "    {'learning_rate': 0.001, 'batch_size': 32},\n",
        "    {'learning_rate': 0.0005, 'batch_size': 64},\n",
        "]\n",
        "\n",
        "best_score = 0\n",
        "best_config = None\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"Testing LR={config['learning_rate']}, Batch={config['batch_size']}\")\n",
        "\n",
        "    model = create_nn_model(X_train_scaled.shape[1], config['learning_rate'])\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        epochs=30,\n",
        "        batch_size=config['batch_size'],\n",
        "        validation_split=0.2,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)\n",
        "        ],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    _, _, val_auc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "    print(f\"  Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "    if val_auc > best_score:\n",
        "        best_score = val_auc\n",
        "        best_config = config\n",
        "        best_model = model\n",
        "\n",
        "print(f\"\\nBest Config: LR={best_config['learning_rate']}, Batch={best_config['batch_size']}\")\n",
        "\n",
        "# Train final model\n",
        "print(\"\\nTraining final model...\")\n",
        "nn_final = create_nn_model(X_train_scaled.shape[1], best_config['learning_rate'])\n",
        "\n",
        "history = nn_final.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=80,\n",
        "    batch_size=best_config['batch_size'],\n",
        "    validation_split=0.2,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=15, min_delta=0.001, restore_best_weights=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, verbose=1)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_proba_nn = nn_final.predict(X_test_scaled).flatten()\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_nn)\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"\\nOptimal threshold: {optimal_threshold:.3f}\")\n",
        "\n",
        "y_pred_nn = (y_pred_proba_nn > optimal_threshold).astype(int)\n",
        "\n",
        "print(\"\\n--- NEURAL NETWORK RESULTS ---\")\n",
        "print(classification_report(y_test, y_pred_nn, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
        "print(f\"Confusion Matrix:\\n{cm_nn}\")\n",
        "\n",
        "nn_metrics = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_nn),\n",
        "    'Precision': precision_score(y_test, y_pred_nn),\n",
        "    'Recall': recall_score(y_test, y_pred_nn),\n",
        "    'F1-Score': f1_score(y_test, y_pred_nn),\n",
        "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_nn)\n",
        "}\n",
        "\n",
        "print(\"\\nMetrics:\")\n",
        "for metric, value in nn_metrics.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "# Training History\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].plot(history.history['loss'], label='Train')\n",
        "axes[0].plot(history.history['val_loss'], label='Validation')\n",
        "axes[0].set_title('Loss', fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(history.history['accuracy'], label='Train')\n",
        "axes[1].plot(history.history['val_accuracy'], label='Validation')\n",
        "axes[1].set_title('Accuracy', fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(history.history['auc'], label='Train')\n",
        "axes[2].plot(history.history['val_auc'], label='Validation')\n",
        "axes[2].set_title('AUC', fontweight='bold')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EtPWqdVDszwO",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}