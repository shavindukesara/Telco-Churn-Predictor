{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTrVhYVqo8Fm6ptzw2bcIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shavindukesara/Telco-Churn-Predictor/blob/main/Telco%20Churn%20Predictor/source%20code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGtSMfsNtKdw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# TELCO CUSTOMER CHURN PREDICTION\n",
        "# ================================================================================\n",
        "\n",
        "!pip install -q scikit-learn tensorflow pandas numpy matplotlib seaborn\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                            accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, roc_curve, auc)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "\n",
        "# Load dataset from URL\n",
        "url = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(f\"Dataset loaded: {df.shape}\")\n",
        "print(df.head())\n",
        "\n",
        "# ================================================================================\n",
        "# EXPLORATORY DATA ANALYSIS (EDA)\n",
        "# ================================================================================\n",
        "\n",
        "# Basic Info\n",
        "print(\"\\n--- Dataset Info ---\")\n",
        "df.info()\n",
        "print(f\"\\nDataset shape: {df.shape}\")\n",
        "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
        "\n",
        "# Target Variable Analysis\n",
        "print(\"\\n--- Target Variable Distribution ---\")\n",
        "print(df['Churn'].value_counts())\n",
        "print(f\"\\nChurn Rate: {df['Churn'].value_counts(normalize=True)['Yes']*100:.2f}%\")\n",
        "\n",
        "# Visualization 1: Churn Distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "churn_counts = df['Churn'].value_counts()\n",
        "axes[0].bar(churn_counts.index, churn_counts.values, color=['#2ecc71', '#e74c3c'])\n",
        "axes[0].set_title('Churn Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Count')\n",
        "for i, v in enumerate(churn_counts.values):\n",
        "    axes[0].text(i, v + 50, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "axes[1].pie(churn_counts.values, labels=churn_counts.index, autopct='%1.1f%%',\n",
        "            startangle=90, colors=['#2ecc71', '#e74c3c'])\n",
        "axes[1].set_title('Churn Percentage', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInsight: Imbalanced dataset - 73% No Churn, 27% Churn\")\n",
        "\n",
        "# Numerical Features Analysis\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# Visualization 2: Numerical Features Distribution\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for idx, col in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n",
        "    axes[idx].hist(df[col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
        "    axes[idx].set_title(f'{col} Distribution', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel(col)\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualization 3: Numerical Features vs Churn\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for idx, col in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n",
        "    df.boxplot(column=col, by='Churn', ax=axes[idx])\n",
        "    axes[idx].set_title(f'{col} vs Churn', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Churn')\n",
        "plt.suptitle('')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInsights:\")\n",
        "print(\"- Churned customers have lower tenure (shorter relationship)\")\n",
        "print(\"- Churned customers have higher monthly charges\")\n",
        "print(\"- Churned customers have lower total charges\")\n",
        "\n",
        "# Categorical Features Analysis\n",
        "# Visualization 4: Key Features vs Churn\n",
        "key_features = ['Contract', 'InternetService', 'PaymentMethod', 'OnlineSecurity']\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(key_features):\n",
        "    churn_data = pd.crosstab(df[feature], df['Churn'], normalize='index') * 100\n",
        "    churn_data.plot(kind='bar', ax=axes[idx], color=['#2ecc71', '#e74c3c'])\n",
        "    axes[idx].set_title(f'Churn Rate by {feature}', fontsize=11, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Percentage (%)')\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "    axes[idx].legend(['No Churn', 'Churn'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Findings:\")\n",
        "print(\"- Month-to-month contracts: ~42% churn rate (HIGHEST)\")\n",
        "print(\"- Fiber optic internet: Higher churn rate\")\n",
        "print(\"- Electronic check payment: Higher churn rate\")\n",
        "print(\"- No online security: Higher churn rate\")\n",
        "\n",
        "# Correlation Analysis\n",
        "print(\"\\n--- Correlation Analysis ---\")\n",
        "df_corr = df.copy()\n",
        "\n",
        "# Encode categorical variables for correlation\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "if 'customerID' in categorical_cols:\n",
        "    categorical_cols.remove('customerID')\n",
        "\n",
        "for col in categorical_cols:\n",
        "    df_corr[col] = le.fit_transform(df_corr[col].astype(str))\n",
        "\n",
        "df_corr = df_corr.drop('customerID', axis=1)\n",
        "df_corr['TotalCharges'].fillna(df_corr['TotalCharges'].median(), inplace=True)\n",
        "\n",
        "# Visualization 5: Correlation Heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "correlation_matrix = df_corr.corr()\n",
        "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, square=True)\n",
        "plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Top correlations with Churn\n",
        "churn_corr = correlation_matrix['Churn'].abs().sort_values(ascending=False)\n",
        "print(\"\\nTop 10 Features Correlated with Churn:\")\n",
        "print(churn_corr[1:11])\n",
        "\n",
        "# Visualization 6: Tenure Groups Analysis\n",
        "tenure_bins = [0, 12, 24, 36, 48, 60, 72]\n",
        "df['TenureGroup'] = pd.cut(df['tenure'], bins=tenure_bins)\n",
        "tenure_churn = pd.crosstab(df['TenureGroup'], df['Churn'], normalize='index') * 100\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "tenure_churn.plot(kind='bar', color=['#2ecc71', '#e74c3c'])\n",
        "plt.title('Churn Rate by Tenure Groups', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Tenure (months)')\n",
        "plt.ylabel('Percentage (%)')\n",
        "plt.legend(['No Churn', 'Churn'])\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInsight: Churn rate decreases significantly with tenure\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TASK 1 COMPLETE - EDA INSIGHTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(\"\"\"\n",
        "KEY FINDINGS:\n",
        "1. Dataset is imbalanced (73% No Churn, 27% Churn)\n",
        "2. Contract type is strongest predictor (month-to-month highest churn)\n",
        "3. Tenure inversely correlated with churn\n",
        "4. Monthly charges positively correlated with churn\n",
        "5. Internet service type and payment method affect churn\n",
        "6. Customers without add-on services churn more\n",
        "\"\"\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATA PREPROCESSING\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_processed = df.copy()\n",
        "\n",
        "# Handle missing values\n",
        "df_processed['TotalCharges'] = pd.to_numeric(df_processed['TotalCharges'], errors='coerce')\n",
        "print(f\"Missing TotalCharges: {df_processed['TotalCharges'].isnull().sum()}\")\n",
        "\n",
        "# direct assignment instead of inplace=True\n",
        "df_processed['TotalCharges'] = df_processed['TotalCharges'].fillna(df_processed['TotalCharges'].median())\n",
        "\n",
        "# Drop non-predictive columns\n",
        "df_processed = df_processed.drop(['customerID', 'TenureGroup'], axis=1, errors='ignore')\n",
        "\n",
        "# Encode binary variables\n",
        "binary_cols = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
        "for col in binary_cols:\n",
        "    df_processed[col] = df_processed[col].map({'Yes': 1, 'No': 0, 'Male': 1, 'Female': 0})\n",
        "\n",
        "multi_class_cols = ['MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                    'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
        "                    'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n",
        "df_processed = pd.get_dummies(df_processed, columns=multi_class_cols, drop_first=True, dtype=int)\n",
        "\n",
        "# Encode target\n",
        "df_processed['Churn'] = df_processed['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Separate features and target\n",
        "X = df_processed.drop('Churn', axis=1)\n",
        "y = df_processed['Churn']\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target distribution:\\n{y.value_counts()}\")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"Training set: {X_train.shape}\")\n",
        "print(f\"Test set: {X_test.shape}\")\n",
        "print(\"Preprocessing complete\")\n",
        "\n",
        "# =============================================================================\n",
        "# DECISION TREE MODEL\n",
        "# =============================================================================\n",
        "\n",
        "# Hyperparameter tuning\n",
        "param_grid = {\n",
        "    'max_depth': [5, 10, 15, 20, None],\n",
        "    'min_samples_split': [2, 5, 10, 20],\n",
        "    'min_samples_leaf': [1, 2, 4, 8],\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5,\n",
        "    scoring='roc_auc',\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nBest Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best CV ROC-AUC: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "#final model training\n",
        "dt_final = grid_search.best_estimator_\n",
        "y_pred_dt = dt_final.predict(X_test)\n",
        "y_pred_proba_dt = dt_final.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"\\n--- DECISION TREE RESULTS ---\")\n",
        "print(classification_report(y_test, y_pred_dt, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "print(f\"Confusion Matrix:\\n{cm_dt}\")\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "plt.ylabel('True Label', fontweight='bold')\n",
        "plt.xlabel('Predicted Label', fontweight='bold')\n",
        "plt.title('Confusion Matrix Heatmap - Decision Tree', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "dt_metrics = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_dt),\n",
        "    'Precision': precision_score(y_test, y_pred_dt),\n",
        "    'Recall': recall_score(y_test, y_pred_dt),\n",
        "    'F1-Score': f1_score(y_test, y_pred_dt),\n",
        "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_dt)\n",
        "}\n",
        "\n",
        "print(\"\\nMetrics:\")\n",
        "for metric, value in dt_metrics.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "# Feature Importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': dt_final.feature_importances_\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Important Features:\")\n",
        "print(feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "# Visualize Feature Importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_15 = feature_importance.head(15)\n",
        "plt.barh(range(len(top_15)), top_15['Importance'], color='steelblue')\n",
        "plt.yticks(range(len(top_15)), top_15['Feature'])\n",
        "plt.xlabel('Importance')\n",
        "plt.title('Top 15 Feature Importances - Decision Tree', fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize Tree\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt_final, max_depth=3, feature_names=X.columns,\n",
        "          class_names=['No Churn', 'Churn'], filled=True, rounded=True, fontsize=9)\n",
        "plt.title('Decision Tree (First 3 Levels)', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# NEURAL NETWORK MODEL\n",
        "# =============================================================================\n",
        "\n",
        "# Calculating class weights\n",
        "class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {i: class_weights_array[i] for i in range(len(class_weights_array))}\n",
        "print(f\"Class Weights: {class_weights}\")\n",
        "\n",
        "# Build model\n",
        "def create_nn_model(input_dim, learning_rate=0.001):\n",
        "    model = Sequential([\n",
        "        Dense(64, activation='relu', input_dim=input_dim),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# Hyperparameter tuning\n",
        "print(\"\\nHyperparameter Tuning...\")\n",
        "configs = [\n",
        "    {'learning_rate': 0.001, 'batch_size': 32},\n",
        "    {'learning_rate': 0.0005, 'batch_size': 64},\n",
        "]\n",
        "\n",
        "best_score = 0\n",
        "best_config = None\n",
        "\n",
        "for config in configs:\n",
        "    print(f\"Testing LR={config['learning_rate']}, Batch={config['batch_size']}\")\n",
        "\n",
        "    model = create_nn_model(X_train_scaled.shape[1], config['learning_rate'])\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train_scaled, y_train,\n",
        "        epochs=30,\n",
        "        batch_size=config['batch_size'],\n",
        "        validation_split=0.2,\n",
        "        class_weight=class_weights,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)\n",
        "        ],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    _, _, val_auc = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
        "    print(f\"  Validation AUC: {val_auc:.4f}\")\n",
        "\n",
        "    if val_auc > best_score:\n",
        "        best_score = val_auc\n",
        "        best_config = config\n",
        "        best_model = model\n",
        "\n",
        "print(f\"\\nBest Config: LR={best_config['learning_rate']}, Batch={best_config['batch_size']}\")\n",
        "\n",
        "# Train final model\n",
        "print(\"\\nTraining final model...\")\n",
        "nn_final = create_nn_model(X_train_scaled.shape[1], best_config['learning_rate'])\n",
        "\n",
        "history = nn_final.fit(\n",
        "    X_train_scaled, y_train,\n",
        "    epochs=80,\n",
        "    batch_size=best_config['batch_size'],\n",
        "    validation_split=0.2,\n",
        "    class_weight=class_weights,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=15, min_delta=0.001, restore_best_weights=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, verbose=1)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate\n",
        "y_pred_proba_nn = nn_final.predict(X_test_scaled).flatten()\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba_nn)\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "print(f\"\\nOptimal threshold: {optimal_threshold:.3f}\")\n",
        "\n",
        "y_pred_nn = (y_pred_proba_nn > optimal_threshold).astype(int)\n",
        "\n",
        "print(\"\\n--- NEURAL NETWORK RESULTS ---\")\n",
        "print(classification_report(y_test, y_pred_nn, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "cm_nn = confusion_matrix(y_test, y_pred_nn)\n",
        "print(f\"Confusion Matrix:\\n{cm_nn}\")\n",
        "\n",
        "nn_metrics = {\n",
        "    'Accuracy': accuracy_score(y_test, y_pred_nn),\n",
        "    'Precision': precision_score(y_test, y_pred_nn),\n",
        "    'Recall': recall_score(y_test, y_pred_nn),\n",
        "    'F1-Score': f1_score(y_test, y_pred_nn),\n",
        "    'ROC-AUC': roc_auc_score(y_test, y_pred_proba_nn)\n",
        "}\n",
        "\n",
        "print(\"\\nMetrics:\")\n",
        "for metric, value in nn_metrics.items():\n",
        "    print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "# Training History\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "axes[0].plot(history.history['loss'], label='Train')\n",
        "axes[0].plot(history.history['val_loss'], label='Validation')\n",
        "axes[0].set_title('Loss', fontweight='bold')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(history.history['accuracy'], label='Train')\n",
        "axes[1].plot(history.history['val_accuracy'], label='Validation')\n",
        "axes[1].set_title('Accuracy', fontweight='bold')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(history.history['auc'], label='Train')\n",
        "axes[2].plot(history.history['val_auc'], label='Validation')\n",
        "axes[2].set_title('AUC', fontweight='bold')\n",
        "axes[2].set_xlabel('Epoch')\n",
        "axes[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve for Neural Network\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
        "         label=f'Neural Network ROC (AUC = {nn_metrics[\"ROC-AUC\"]:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100,\n",
        "            label=f'Optimal Threshold: {optimal_threshold:.3f}')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontweight='bold')\n",
        "plt.title('ROC Curve - Neural Network', fontweight='bold')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# MODEL COMPARISON\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MODEL COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Comparison table\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Metric': ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC'],\n",
        "    'Decision Tree': [dt_metrics['Accuracy'], dt_metrics['Precision'],\n",
        "                      dt_metrics['Recall'], dt_metrics['F1-Score'], dt_metrics['ROC-AUC']],\n",
        "    'Neural Network': [nn_metrics['Accuracy'], nn_metrics['Precision'],\n",
        "                       nn_metrics['Recall'], nn_metrics['F1-Score'], nn_metrics['ROC-AUC']]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + comparison_df.to_string(index=False))\n",
        "\n",
        "# Visualization: Metrics Comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "x = np.arange(len(comparison_df['Metric']))\n",
        "width = 0.35\n",
        "\n",
        "bars1 = axes[0].bar(x - width/2, comparison_df['Decision Tree'], width,\n",
        "                    label='Decision Tree', color='steelblue')\n",
        "bars2 = axes[0].bar(x + width/2, comparison_df['Neural Network'], width,\n",
        "                    label='Neural Network', color='coral')\n",
        "\n",
        "axes[0].set_xlabel('Metric', fontweight='bold')\n",
        "axes[0].set_ylabel('Score', fontweight='bold')\n",
        "axes[0].set_title('Model Performance Comparison', fontweight='bold')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(comparison_df['Metric'])\n",
        "axes[0].legend()\n",
        "axes[0].set_ylim(0, 1)\n",
        "\n",
        "for bars in [bars1, bars2]:\n",
        "    for bar in bars:\n",
        "        height = bar.get_height()\n",
        "        axes[0].text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Confusion Matrices\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Plot Decision Tree Confusion Matrix\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "axes[0].set_title('Decision Tree - Confusion Matrix', fontweight='bold')\n",
        "axes[0].set_ylabel('True Label')\n",
        "axes[0].set_xlabel('Predicted Label')\n",
        "\n",
        "# Plot Neural Network Confusion Matrix\n",
        "sns.heatmap(cm_nn, annot=True, fmt='d', cmap='Reds', ax=axes[1],\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "axes[1].set_title('Neural Network - Confusion Matrix', fontweight='bold')\n",
        "axes[1].set_ylabel('True Label')\n",
        "axes[1].set_xlabel('Predicted Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ROC Curves\n",
        "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba_dt)\n",
        "fpr_nn, tpr_nn, _ = roc_curve(y_test, y_pred_proba_nn)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_dt, tpr_dt, linewidth=3, label=f'Decision Tree (AUC={dt_metrics[\"ROC-AUC\"]:.3f})')\n",
        "plt.plot(fpr_nn, tpr_nn, linewidth=3, label=f'Neural Network (AUC={nn_metrics[\"ROC-AUC\"]:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.xlabel('False Positive Rate', fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontweight='bold')\n",
        "plt.title('ROC Curve Comparison', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "winner = 'Neural Network' if nn_metrics['ROC-AUC'] > dt_metrics['ROC-AUC'] else 'Decision Tree'\n",
        "print(f\"\\nBest Model: {winner}\")\n",
        "print(f\"\\nDecision Tree - ROC-AUC: {dt_metrics['ROC-AUC']:.4f}\")\n",
        "print(f\"Neural Network - ROC-AUC: {nn_metrics['ROC-AUC']:.4f}\")\n",
        "\n"
      ]
    }
  ]
}