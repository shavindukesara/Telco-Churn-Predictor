{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNpLcufmet+AIvvD0dU1ACW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shavindukesara/Telco-Churn-Predictor/blob/main/Telco%20Churn%20Predictor/source%20code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGtSMfsNtKdw",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# ================================================================================\n",
        "# TELCO CUSTOMER CHURN PREDICTION SYSTEM\n",
        "# ================================================================================\n",
        "\n",
        "# Install required computational packages for data analysis\n",
        "!pip install -q scikit-learn tensorflow pandas numpy matplotlib seaborn\n",
        "\n",
        "# Import core data processing and visualization modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import machine learning components from scikit-learn\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import (classification_report, confusion_matrix,\n",
        "                            accuracy_score, precision_score, recall_score,\n",
        "                            f1_score, roc_auc_score, roc_curve, auc)\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Import deep learning framework components\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Set random seeds for reproducible analytical results\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "print(\"All computational libraries imported and configured successfully!\")\n",
        "\n",
        "# ================================================================================\n",
        "# LOAD TELCO CUSTOMER DATASET FOR ANALYSIS\n",
        "# ================================================================================\n",
        "\n",
        "# Retrieve dataset from IBM's public repository\n",
        "dataset_url = 'https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/master/data/Telco-Customer-Churn.csv'\n",
        "customer_records = pd.read_csv(dataset_url)\n",
        "\n",
        "print(f\"Customer dataset loaded with dimensions: {customer_records.shape}\")\n",
        "print(customer_records.head())\n",
        "\n",
        "# ================================================================================\n",
        "# EXPLORATORY DATA ANALYSIS: CUSTOMER BEHAVIOR PATTERNS\n",
        "# ================================================================================\n",
        "\n",
        "# Display dataset metadata and structural information\n",
        "print(\"\\n--- Dataset Structural Overview ---\")\n",
        "customer_records.info()\n",
        "print(f\"\\nDataset dimensions: {customer_records.shape}\")\n",
        "print(f\"\\nMissing data points across columns:\\n{customer_records.isnull().sum()}\")\n",
        "\n",
        "# Analyze distribution of the target variable\n",
        "print(\"\\n--- Customer Churn Distribution Analysis ---\")\n",
        "print(customer_records['Churn'].value_counts())\n",
        "print(f\"\\nOverall churn percentage: {customer_records['Churn'].value_counts(normalize=True)['Yes']*100:.2f}%\")\n",
        "\n",
        "# Visual representation of churn distribution\n",
        "fig, chart_panels = plt.subplots(1, 2, figsize=(14, 5))\n",
        "churn_distribution = customer_records['Churn'].value_counts()\n",
        "chart_panels[0].bar(churn_distribution.index, churn_distribution.values, color=['#2ecc71', '#e74c3c'])\n",
        "chart_panels[0].set_title('Customer Churn Distribution', fontsize=14, fontweight='bold')\n",
        "chart_panels[0].set_ylabel('Customer Count')\n",
        "for position, count_value in enumerate(churn_distribution.values):\n",
        "    chart_panels[0].text(position, count_value + 50, str(count_value), ha='center', fontweight='bold')\n",
        "\n",
        "chart_panels[1].pie(churn_distribution.values, labels=churn_distribution.index, autopct='%1.1f%%',\n",
        "            startangle=90, colors=['#2ecc71', '#e74c3c'])\n",
        "chart_panels[1].set_title('Churn Percentage Breakdown', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservation: Dataset exhibits class imbalance - 73% retained customers vs 27% churned customers\")\n",
        "\n",
        "# ================================================================================\n",
        "# NUMERICAL ATTRIBUTE ANALYSIS\n",
        "# ================================================================================\n",
        "\n",
        "print(f\"\\nMissing entries in TotalCharges before conversion: {customer_records['TotalCharges'].isnull().sum()}\")\n",
        "customer_records['TotalCharges'] = pd.to_numeric(customer_records['TotalCharges'], errors='coerce')\n",
        "print(f\"Missing entries in TotalCharges after conversion: {customer_records['TotalCharges'].isnull().sum()}\")\n",
        "\n",
        "# Prepare clean dataset for visualization purposes\n",
        "clean_dataset = customer_records.dropna(subset=['TotalCharges'])\n",
        "\n",
        "# Visualize distribution of numerical attributes\n",
        "fig, attribute_plots = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for plot_index, numerical_feature in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n",
        "    attribute_plots[plot_index].hist(clean_dataset[numerical_feature], bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "    attribute_plots[plot_index].set_title(f'{numerical_feature} Distribution Pattern', fontsize=12, fontweight='bold')\n",
        "    attribute_plots[plot_index].set_xlabel(numerical_feature)\n",
        "    attribute_plots[plot_index].set_ylabel('Frequency Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Comparative analysis of numerical features by churn status\n",
        "fig, comparative_plots = plt.subplots(1, 3, figsize=(18, 5))\n",
        "for plot_index, numerical_attribute in enumerate(['tenure', 'MonthlyCharges', 'TotalCharges']):\n",
        "    customer_records.boxplot(column=numerical_attribute, by='Churn', ax=comparative_plots[plot_index], patch_artist=True,\n",
        "               boxprops=dict(facecolor='lightblue'),\n",
        "               medianprops=dict(color='red', linewidth=2))\n",
        "    comparative_plots[plot_index].set_title(f'{numerical_attribute} by Churn Status', fontsize=12, fontweight='bold')\n",
        "    comparative_plots[plot_index].set_xlabel('Churn Category')\n",
        "    comparative_plots[plot_index].set_ylabel(numerical_attribute)\n",
        "    comparative_plots[plot_index].set_title(f'{numerical_attribute} Distribution by Churn Status', fontsize=12, fontweight='bold')\n",
        "\n",
        "plt.suptitle('')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey numerical insights:\")\n",
        "print(\"- Customers who churn typically have shorter relationship duration\")\n",
        "print(\"- Churned customers often have higher monthly service fees\")\n",
        "print(\"- Total charges are generally lower for customers who eventually churn\")\n",
        "\n",
        "# ================================================================================\n",
        "# CATEGORICAL ATTRIBUTE ANALYSIS\n",
        "# ================================================================================\n",
        "\n",
        "# Analyze key categorical features affecting churn\n",
        "significant_categories = ['Contract', 'InternetService', 'PaymentMethod', 'OnlineSecurity']\n",
        "fig, category_panels = plt.subplots(2, 2, figsize=(16, 10))\n",
        "category_panels = category_panels.ravel()\n",
        "\n",
        "for panel_index, categorical_feature in enumerate(significant_categories):\n",
        "    churn_by_category = pd.crosstab(customer_records[categorical_feature], customer_records['Churn'], normalize='index') * 100\n",
        "    churn_by_category.plot(kind='bar', ax=category_panels[panel_index], color=['#2ecc71', '#e74c3c'])\n",
        "    category_panels[panel_index].set_title(f'Churn Rate Analysis: {categorical_feature}', fontsize=11, fontweight='bold')\n",
        "    category_panels[panel_index].set_ylabel('Percentage (%)')\n",
        "    category_panels[panel_index].tick_params(axis='x', rotation=45)\n",
        "    category_panels[panel_index].legend(['No Churn', 'Churn'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nCritical categorical findings:\")\n",
        "print(\"- Month-to-month contracts show highest churn rate at ~42%\")\n",
        "print(\"- Fiber optic internet subscribers have elevated churn likelihood\")\n",
        "print(\"- Electronic check payment method correlates with higher churn\")\n",
        "print(\"- Customers without online security features churn more frequently\")\n",
        "\n",
        "# ================================================================================\n",
        "# ATTRIBUTE CORRELATION ANALYSIS\n",
        "# ================================================================================\n",
        "\n",
        "print(\"\\n--- Feature Correlation Examination ---\")\n",
        "encoded_dataset = customer_records.copy()\n",
        "\n",
        "# Prepare categorical features for correlation analysis\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "category_encoder = LabelEncoder()\n",
        "\n",
        "categorical_features = encoded_dataset.select_dtypes(include=['object']).columns.tolist()\n",
        "if 'customerID' in categorical_features:\n",
        "    categorical_features.remove('customerID')\n",
        "\n",
        "for feature_column in categorical_features:\n",
        "    encoded_dataset[feature_column] = category_encoder.fit_transform(encoded_dataset[feature_column].astype(str))\n",
        "\n",
        "encoded_dataset = encoded_dataset.drop('customerID', axis=1)\n",
        "encoded_dataset['TotalCharges'] = pd.to_numeric(encoded_dataset['TotalCharges'], errors='coerce')\n",
        "encoded_dataset['TotalCharges'] = encoded_dataset['TotalCharges'].fillna(encoded_dataset['TotalCharges'].median())\n",
        "\n",
        "correlation_structure = encoded_dataset.corr()\n",
        "\n",
        "# Identify features most correlated with churn\n",
        "churn_correlations = correlation_structure['Churn'].abs().sort_values(ascending=False)\n",
        "top_correlated_features = churn_correlations.head(15).index.tolist()\n",
        "top_correlation_view = correlation_structure.loc[top_correlated_features, top_correlated_features]\n",
        "\n",
        "# Visualize correlation patterns\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(top_correlation_view,\n",
        "            annot=True,\n",
        "            fmt='.2f',\n",
        "            cmap='RdYlBu_r',\n",
        "            center=0,\n",
        "            square=True,\n",
        "            linewidths=0.5,\n",
        "            linecolor='white',\n",
        "            cbar_kws={'shrink': 0.8, 'label': 'Correlation Coefficient'},\n",
        "            annot_kws={'size': 9, 'weight': 'bold'})\n",
        "\n",
        "plt.title('Top 15 Feature Correlations with Churn', fontsize=16, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.yticks(rotation=0, fontsize=10)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTop 15 attributes correlated with churn:\")\n",
        "print(churn_correlations[1:16])\n",
        "\n",
        "# ================================================================================\n",
        "# TENURE SEGMENTATION ANALYSIS\n",
        "# ================================================================================\n",
        "\n",
        "# Create tenure segments for detailed analysis\n",
        "tenure_intervals = [0, 12, 24, 36, 48, 60, 72]\n",
        "customer_records['TenureSegment'] = pd.cut(customer_records['tenure'], bins=tenure_intervals, labels=['0-12', '13-24', '25-36', '37-48', '49-60', '61-72'])\n",
        "churn_by_tenure = pd.crosstab(customer_records['TenureSegment'], customer_records['Churn'], normalize='index') * 100\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "churn_by_tenure.plot(kind='bar', color=['#2ecc71', '#e74c3c'])\n",
        "plt.title('Churn Analysis by Tenure Duration', fontsize=14, fontweight='bold')\n",
        "plt.xlabel('Customer Tenure (months)')\n",
        "plt.ylabel('Percentage Distribution (%)')\n",
        "plt.legend(['No Churn', 'Churn'])\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nInsight: Churn probability decreases significantly as customer tenure increases\")\n",
        "\n",
        "print(\"\"\"\n",
        "COMPREHENSIVE ANALYSIS FINDINGS:\n",
        "1. Dataset exhibits significant class imbalance favoring retained customers\n",
        "2. Contract type emerges as the most influential churn predictor\n",
        "3. Customer tenure shows strong inverse relationship with churn likelihood\n",
        "4. Monthly charges demonstrate positive correlation with churn probability\n",
        "5. Service type and payment method substantially impact retention rates\n",
        "6. Additional services play protective role against customer attrition\n",
        "\"\"\")\n",
        "\n",
        "# =============================================================================\n",
        "# DATA PREPARATION PIPELINE\n",
        "# =============================================================================\n",
        "\n",
        "prepared_data = customer_records.copy()\n",
        "\n",
        "# Handle missing numerical values\n",
        "prepared_data['TotalCharges'] = pd.to_numeric(prepared_data['TotalCharges'], errors='coerce')\n",
        "print(f\"Missing TotalCharges entries: {prepared_data['TotalCharges'].isnull().sum()}\")\n",
        "\n",
        "prepared_data['TotalCharges'] = prepared_data['TotalCharges'].fillna(prepared_data['TotalCharges'].median())\n",
        "\n",
        "# Remove non-predictive identifier columns\n",
        "prepared_data = prepared_data.drop(['customerID', 'TenureSegment'], axis=1, errors='ignore')\n",
        "\n",
        "# Convert binary categorical features to numerical format\n",
        "binary_features = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
        "for binary_column in binary_features:\n",
        "    prepared_data[binary_column] = prepared_data[binary_column].map({'Yes': 1, 'No': 0, 'Male': 1, 'Female': 0})\n",
        "\n",
        "# Apply one-hot encoding to multi-class categorical features\n",
        "multi_category_features = ['MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "                    'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
        "                    'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n",
        "prepared_data = pd.get_dummies(prepared_data, columns=multi_category_features, drop_first=True, dtype=int)\n",
        "\n",
        "# Encode target variable for modeling\n",
        "prepared_data['Churn'] = prepared_data['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "# Separate predictive features from target variable\n",
        "predictor_variables = prepared_data.drop('Churn', axis=1)\n",
        "target_variable = prepared_data['Churn']\n",
        "\n",
        "print(f\"Predictor feature dimensions: {predictor_variables.shape}\")\n",
        "print(f\"Target variable distribution:\\n{target_variable.value_counts()}\")\n",
        "\n",
        "# Split data into training and testing subsets\n",
        "X_train_set, X_test_set, y_train_set, y_test_set = train_test_split(\n",
        "    predictor_variables, target_variable, test_size=0.2, random_state=42, stratify=target_variable\n",
        ")\n",
        "\n",
        "# Standardize features for neural network compatibility\n",
        "standard_scaler = StandardScaler()\n",
        "X_train_normalized = standard_scaler.fit_transform(X_train_set)\n",
        "X_test_normalized = standard_scaler.transform(X_test_set)\n",
        "\n",
        "print(f\"Training subset dimensions: {X_train_set.shape}\")\n",
        "print(f\"Testing subset dimensions: {X_test_set.shape}\")\n",
        "print(\"Data preparation pipeline completed successfully\")\n",
        "\n",
        "# =============================================================================\n",
        "# DECISION TREE CLASSIFICATION MODEL\n",
        "# =============================================================================\n",
        "\n",
        "# Configure hyperparameter search space\n",
        "parameter_grid = {\n",
        "    'max_depth': [4, 6, 8, 10, 12], # Tree complexity control\n",
        "    'min_samples_split': [10, 20, 30, 40], # Node splitting criteria\n",
        "    'min_samples_leaf': [5, 10, 15, 20], # Terminal node control\n",
        "    'criterion': ['gini', 'entropy'], # Impurity measures\n",
        "    'max_features': ['sqrt', 'log2', 0.5, 0.7, None] # Feature sampling\n",
        "}\n",
        "\n",
        "# Execute comprehensive hyperparameter optimization\n",
        "grid_optimization_process = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    parameter_grid,\n",
        "    cv=5, # 5-fold cross-validation\n",
        "    scoring='roc_auc', # Primary optimization metric\n",
        "    n_jobs=-1, # Parallel computation\n",
        "    verbose=1 # Search progress tracking\n",
        ")\n",
        "\n",
        "grid_optimization_process.fit(X_train_set, y_train_set)\n",
        "\n",
        "print(f\"\\nOptimal parameter configuration: {grid_optimization_process.best_params_}\")\n",
        "print(f\"Best cross-validation ROC-AUC score: {grid_optimization_process.best_score_:.4f}\")\n",
        "\n",
        "# Extract best performing decision tree model\n",
        "optimal_tree_model = grid_optimization_process.best_estimator_\n",
        "tree_predictions = optimal_tree_model.predict(X_test_set)\n",
        "tree_probability_predictions = optimal_tree_model.predict_proba(X_test_set)[:, 1]\n",
        "\n",
        "print(\"\\n--- DECISION TREE MODEL PERFORMANCE ---\")\n",
        "print(classification_report(y_test_set, tree_predictions, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "tree_confusion_results = confusion_matrix(y_test_set, tree_predictions)\n",
        "print(f\"Confusion matrix results:\\n{tree_confusion_results}\")\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(tree_confusion_results, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "plt.ylabel('Actual Classification', fontweight='bold')\n",
        "plt.xlabel('Predicted Classification', fontweight='bold')\n",
        "plt.title('Decision Tree Confusion Matrix Visualization', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "tree_performance_metrics = {\n",
        "    'Accuracy Score': accuracy_score(y_test_set, tree_predictions),\n",
        "    'Precision Metric': precision_score(y_test_set, tree_predictions),\n",
        "    'Recall Metric': recall_score(y_test_set, tree_predictions),\n",
        "    'F1 Score': f1_score(y_test_set, tree_predictions),\n",
        "    'ROC-AUC Score': roc_auc_score(y_test_set, tree_probability_predictions)\n",
        "}\n",
        "\n",
        "print(\"\\nModel performance metrics:\")\n",
        "for metric_name, metric_value in tree_performance_metrics.items():\n",
        "    print(f\"  {metric_name}: {metric_value:.4f}\")\n",
        "\n",
        "# Analyze feature importance in decision tree\n",
        "feature_importance_analysis = pd.DataFrame({\n",
        "    'Predictor Feature': X_train_set.columns.tolist(),\n",
        "    'Importance Value': optimal_tree_model.feature_importances_\n",
        "}).sort_values('Importance Value', ascending=False)\n",
        "\n",
        "print(f\"\\nFeatures with predictive power: {(feature_importance_analysis['Importance Value'] > 0).sum()}/{len(feature_importance_analysis)}\")\n",
        "print(f\"Total importance distribution: {feature_importance_analysis['Importance Value'].sum():.6f}\")\n",
        "\n",
        "print(\"\\nTop 10 influential features:\")\n",
        "print(feature_importance_analysis.head(10).to_string(index=False))\n",
        "\n",
        "print(\"\\nContract type impact interpretation:\")\n",
        "for contract_type in ['Contract_Two year', 'Contract_One year']:\n",
        "    if contract_type in feature_importance_analysis['Predictor Feature'].values:\n",
        "        importance_value = feature_importance_analysis.loc[feature_importance_analysis['Predictor Feature'] == contract_type, 'Importance Value'].values[0]\n",
        "        print(f\"  {contract_type}: {importance_value:.4f} (compared to month-to-month baseline)\")\n",
        "\n",
        "# Visualize top feature importance\n",
        "plt.figure(figsize=(14, 8))\n",
        "display_count = min(15, len(feature_importance_analysis))\n",
        "top_important_features = feature_importance_analysis.head(display_count)\n",
        "\n",
        "color_scheme = []\n",
        "for feature_name in top_important_features['Predictor Feature']:\n",
        "    if 'Contract' in feature_name:\n",
        "        color_scheme.append('#e74c3c')\n",
        "    elif 'tenure' in feature_name.lower():\n",
        "        color_scheme.append('#3498db')\n",
        "    elif 'charge' in feature_name.lower():\n",
        "        color_scheme.append('#2ecc71')\n",
        "    else:\n",
        "        color_scheme.append('#95a5a6')\n",
        "\n",
        "importance_bars = plt.barh(range(display_count), top_important_features['Importance Value'], color=color_scheme, edgecolor='black')\n",
        "\n",
        "for bar_position, (importance_val, feature_name) in enumerate(zip(top_important_features['Importance Value'], top_important_features['Predictor Feature'])):\n",
        "    plt.text(importance_val + 0.001, bar_position, f'{importance_val:.4f}', va='center', fontsize=9)\n",
        "\n",
        "plt.yticks(range(display_count), top_important_features['Predictor Feature'])\n",
        "plt.xlabel('Feature Importance Score', fontweight='bold')\n",
        "plt.title(f'Top {display_count} Feature Importance Rankings', fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.grid(axis='x', alpha=0.3, linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize decision tree structure\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(optimal_tree_model, max_depth=3, feature_names=X_train_set.columns.tolist(),\n",
        "          class_names=['No Churn', 'Churn'], filled=True, rounded=True, fontsize=9)\n",
        "plt.title('Decision Tree Structure Visualization (Top 3 Levels)', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Decision tree structural analysis\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DECISION TREE STRUCTURAL ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"Tree depth configuration: {optimal_tree_model.get_depth()}\")\n",
        "print(f\"Number of terminal nodes: {optimal_tree_model.get_n_leaves()}\")\n",
        "\n",
        "# Examine tree splitting structure\n",
        "tree_architecture = optimal_tree_model.tree_\n",
        "feature_split_indices = tree_architecture.feature\n",
        "print(\"\\nInitial splitting decisions in tree:\")\n",
        "for node_index in range(min(5, tree_architecture.node_count)):\n",
        "    if feature_split_indices[node_index] != -2:\n",
        "        split_feature = X_train_set.columns[feature_split_indices[node_index]]\n",
        "        split_threshold = tree_architecture.threshold[node_index]\n",
        "        print(f\"  Node {node_index}: Split on {split_feature} <= {split_threshold:.4f}\")\n",
        "\n",
        "# Identify features used in tree construction\n",
        "print(\"\\nFeatures utilized in tree construction:\")\n",
        "utilized_features = []\n",
        "for node_index in range(tree_architecture.node_count):\n",
        "    if feature_split_indices[node_index] != -2:\n",
        "        split_feature = X_train_set.columns[feature_split_indices[node_index]]\n",
        "        if split_feature not in utilized_features:\n",
        "            utilized_features.append(split_feature)\n",
        "\n",
        "print(f\"Unique features in decision tree: {len(utilized_features)}\")\n",
        "print(\"Primary splitting features:\")\n",
        "for feature_item in utilized_features[:5]:\n",
        "    print(f\"  â€¢ {feature_item}\")\n",
        "\n",
        "#Scaled vs Unscaled Analysis\n",
        "# =============================================================================\n",
        "# DECISION TREE ON NORMALIZED DATA (Comparative Analysis)\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DECISION TREE PERFORMANCE ON NORMALIZED DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Train decision tree on standardized features\n",
        "normalized_tree_model = DecisionTreeClassifier(\n",
        "    **grid_optimization_process.best_params_,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "normalized_tree_model.fit(X_train_normalized, y_train_set)\n",
        "\n",
        "# Generate predictions from normalized data model\n",
        "normalized_tree_predictions = normalized_tree_model.predict(X_test_normalized)\n",
        "normalized_tree_probabilities = normalized_tree_model.predict_proba(X_test_normalized)[:, 1]\n",
        "\n",
        "print(\"\\n--- NORMALIZED DATA DECISION TREE RESULTS ---\")\n",
        "print(classification_report(y_test_set, normalized_tree_predictions, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "normalized_confusion_matrix = confusion_matrix(y_test_set, normalized_tree_predictions)\n",
        "print(f\"Confusion matrix results:\\n{normalized_confusion_matrix}\")\n",
        "\n",
        "# Calculate performance metrics for normalized tree\n",
        "normalized_tree_metrics = {\n",
        "    'Accuracy Score': accuracy_score(y_test_set, normalized_tree_predictions),\n",
        "    'Precision Metric': precision_score(y_test_set, normalized_tree_predictions),\n",
        "    'Recall Metric': recall_score(y_test_set, normalized_tree_predictions),\n",
        "    'F1 Score': f1_score(y_test_set, normalized_tree_predictions),\n",
        "    'ROC-AUC Score': roc_auc_score(y_test_set, normalized_tree_probabilities)\n",
        "}\n",
        "\n",
        "print(\"\\nNormalized tree performance metrics:\")\n",
        "for metric_name, metric_value in normalized_tree_metrics.items():\n",
        "    print(f\"  {metric_name}: {metric_value:.4f}\")\n",
        "\n",
        "# Confusion matrix visualization for normalized tree\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(normalized_confusion_matrix, annot=True, fmt='d', cmap='Greens',\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "plt.ylabel('Actual Classification', fontweight='bold')\n",
        "plt.xlabel('Predicted Classification', fontweight='bold')\n",
        "plt.title('Normalized Data Decision Tree Confusion Matrix', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Feature importance analysis for normalized tree\n",
        "normalized_feature_importance = pd.DataFrame({\n",
        "    'Predictor Feature': X_train_set.columns.tolist(),\n",
        "    'Importance Value': normalized_tree_model.feature_importances_\n",
        "}).sort_values('Importance Value', ascending=False)\n",
        "\n",
        "print(f\"\\nFeatures with predictive power in normalized tree: {(normalized_feature_importance['Importance Value'] > 0).sum()}/{len(normalized_feature_importance)}\")\n",
        "\n",
        "print(\"\\nTop 10 influential features in normalized tree:\")\n",
        "print(normalized_feature_importance.head(10).to_string(index=False))\n",
        "\n",
        "# =============================================================================\n",
        "# ROC ANALYSIS FOR BOTH DECISION TREE MODELS\n",
        "# =============================================================================\n",
        "\n",
        "# Generate ROC curves for comparative analysis\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "# ROC curve for original decision tree\n",
        "fpr_original_tree, tpr_original_tree, _ = roc_curve(y_test_set, tree_probability_predictions)\n",
        "\n",
        "# ROC curve for normalized decision tree\n",
        "fpr_normalized_tree, tpr_normalized_tree, _ = roc_curve(y_test_set, normalized_tree_probabilities)\n",
        "\n",
        "# =============================================================================\n",
        "# COMPARATIVE ANALYSIS: ORIGINAL VS NORMALIZED DATA\n",
        "# =============================================================================\n",
        "\n",
        "tree_comparison_results = pd.DataFrame({\n",
        "    'Performance Metric': ['Accuracy Score', 'Precision Metric', 'Recall Metric', 'F1 Score', 'ROC-AUC Score'],\n",
        "    'Original Data Tree': [tree_performance_metrics['Accuracy Score'], tree_performance_metrics['Precision Metric'],\n",
        "                           tree_performance_metrics['Recall Metric'], tree_performance_metrics['F1 Score'], tree_performance_metrics['ROC-AUC Score']],\n",
        "    'Normalized Data Tree': [normalized_tree_metrics['Accuracy Score'], normalized_tree_metrics['Precision Metric'],\n",
        "                              normalized_tree_metrics['Recall Metric'], normalized_tree_metrics['F1 Score'], normalized_tree_metrics['ROC-AUC Score']]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + tree_comparison_results.to_string(index=False))\n",
        "\n",
        "# Visual comparison of both decision tree models\n",
        "fig, comparison_plots = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Performance metrics bar chart comparison\n",
        "metric_positions = np.arange(len(tree_comparison_results['Performance Metric']))\n",
        "bar_width = 0.35\n",
        "\n",
        "original_bars = comparison_plots[0].bar(metric_positions - bar_width/2, tree_comparison_results['Original Data Tree'], bar_width,\n",
        "                    label='Original Data Model', color='steelblue')\n",
        "normalized_bars = comparison_plots[0].bar(metric_positions + bar_width/2, tree_comparison_results['Normalized Data Tree'], bar_width,\n",
        "                    label='Normalized Data Model', color='lightgreen')\n",
        "\n",
        "comparison_plots[0].set_xlabel('Performance Metric', fontweight='bold')\n",
        "comparison_plots[0].set_ylabel('Metric Value', fontweight='bold')\n",
        "comparison_plots[0].set_title('Decision Tree: Original vs Normalized Data Performance', fontweight='bold')\n",
        "comparison_plots[0].set_xticks(metric_positions)\n",
        "comparison_plots[0].set_xticklabels(tree_comparison_results['Performance Metric'])\n",
        "comparison_plots[0].legend()\n",
        "comparison_plots[0].set_ylim(0, 1)\n",
        "\n",
        "for bar_collection in [original_bars, normalized_bars]:\n",
        "    for individual_bar in bar_collection:\n",
        "        bar_height = individual_bar.get_height()\n",
        "        comparison_plots[0].text(individual_bar.get_x() + individual_bar.get_width()/2., bar_height,\n",
        "                    f'{bar_height:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Feature importance comparison visualization\n",
        "top_features_count = 10\n",
        "top_original_features = feature_importance_analysis.head(top_features_count)\n",
        "top_normalized_features = normalized_feature_importance.head(top_features_count)\n",
        "\n",
        "# Align features for direct comparison\n",
        "common_feature_set = set(top_original_features['Predictor Feature']).union(set(top_normalized_features['Predictor Feature']))\n",
        "importance_comparison = {}\n",
        "for feature_name in common_feature_set:\n",
        "    importance_comparison[feature_name] = {\n",
        "        'Original': top_original_features.loc[top_original_features['Predictor Feature'] == feature_name, 'Importance Value'].values[0]\n",
        "               if feature_name in top_original_features['Predictor Feature'].values else 0,\n",
        "        'Normalized': top_normalized_features.loc[top_normalized_features['Predictor Feature'] == feature_name, 'Importance Value'].values[0]\n",
        "                 if feature_name in top_normalized_features['Predictor Feature'].values else 0\n",
        "    }\n",
        "\n",
        "importance_comparison_df = pd.DataFrame(importance_comparison).T.sort_values('Original', ascending=False)\n",
        "\n",
        "feature_positions = np.arange(len(importance_comparison_df))\n",
        "comparison_plots[1].bar(feature_positions - 0.2, importance_comparison_df['Original'], 0.4, label='Original Data', color='steelblue', alpha=0.8)\n",
        "comparison_plots[1].bar(feature_positions + 0.2, importance_comparison_df['Normalized'], 0.4, label='Normalized Data', color='lightgreen', alpha=0.8)\n",
        "\n",
        "comparison_plots[1].set_xlabel('Predictor Features', fontweight='bold')\n",
        "comparison_plots[1].set_ylabel('Importance Score', fontweight='bold')\n",
        "comparison_plots[1].set_title('Feature Importance Comparison: Original vs Normalized', fontweight='bold')\n",
        "comparison_plots[1].set_xticks(feature_positions)\n",
        "comparison_plots[1].set_xticklabels(importance_comparison_df.index, rotation=45, ha='right')\n",
        "comparison_plots[1].legend()\n",
        "comparison_plots[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ROC curve comparison visualization\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_original_tree, tpr_original_tree, linewidth=3, label=f'Original Data Tree (AUC={tree_performance_metrics[\"ROC-AUC Score\"]:.3f})')\n",
        "plt.plot(fpr_normalized_tree, tpr_normalized_tree, linewidth=3, label=f'Normalized Data Tree (AUC={normalized_tree_metrics[\"ROC-AUC Score\"]:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier Baseline')\n",
        "plt.xlabel('False Positive Rate', fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontweight='bold')\n",
        "plt.title('ROC Curve Comparison: Original vs Normalized Decision Trees', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# NEURAL NETWORK CLASSIFICATION MODEL\n",
        "# =============================================================================\n",
        "\n",
        "# Calculate class weights to address imbalance\n",
        "calculated_class_weights = compute_class_weight('balanced', classes=np.unique(y_train_set), y=y_train_set)\n",
        "class_weight_dict = {i: calculated_class_weights[i] for i in range(len(calculated_class_weights))}\n",
        "print(f\"Calculated class weights: {class_weight_dict}\")\n",
        "\n",
        "# Define neural network architecture\n",
        "def construct_neural_network(input_dimension, learning_rate=0.001):\n",
        "    network_model = Sequential([\n",
        "        Dense(64, activation='relu', input_dim=input_dimension),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(0.5),\n",
        "\n",
        "        Dense(16, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    network_model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]\n",
        "    )\n",
        "    return network_model\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "print(\"\\nNeural Network Hyperparameter Optimization Process...\")\n",
        "parameter_configurations = [\n",
        "    {'learning_rate': 0.001, 'batch_size': 32},\n",
        "    {'learning_rate': 0.0005, 'batch_size': 64},\n",
        "]\n",
        "\n",
        "best_validation_score = 0\n",
        "optimal_configuration = None\n",
        "\n",
        "for config_setting in parameter_configurations:\n",
        "    print(f\"Testing configuration - Learning Rate: {config_setting['learning_rate']}, Batch Size: {config_setting['batch_size']}\")\n",
        "\n",
        "    test_network = construct_neural_network(X_train_normalized.shape[1], config_setting['learning_rate'])\n",
        "\n",
        "    training_history = test_network.fit(\n",
        "        X_train_normalized, y_train_set,\n",
        "        epochs=30,\n",
        "        batch_size=config_setting['batch_size'],\n",
        "        validation_split=0.2,\n",
        "        class_weight=class_weight_dict,\n",
        "        callbacks=[\n",
        "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
        "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=0)\n",
        "        ],\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    _, _, validation_auc_score = test_network.evaluate(X_test_normalized, y_test_set, verbose=0)\n",
        "    print(f\"  Validation AUC performance: {validation_auc_score:.4f}\")\n",
        "\n",
        "    if validation_auc_score > best_validation_score:\n",
        "        best_validation_score = validation_auc_score\n",
        "        optimal_configuration = config_setting\n",
        "        best_network_model = test_network\n",
        "\n",
        "print(f\"\\nOptimal configuration identified: Learning Rate={optimal_configuration['learning_rate']}, Batch Size={optimal_configuration['batch_size']}\")\n",
        "\n",
        "# Train final neural network model\n",
        "print(\"\\nTraining final neural network model...\")\n",
        "final_neural_network = construct_neural_network(X_train_normalized.shape[1], optimal_configuration['learning_rate'])\n",
        "\n",
        "complete_training_history = final_neural_network.fit(\n",
        "    X_train_normalized, y_train_set,\n",
        "    epochs=80,\n",
        "    batch_size=optimal_configuration['batch_size'],\n",
        "    validation_split=0.2,\n",
        "    class_weight=class_weight_dict,\n",
        "    callbacks=[\n",
        "        EarlyStopping(monitor='val_loss', patience=15, min_delta=0.001, restore_best_weights=True, verbose=1),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8, verbose=1)\n",
        "    ],\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate neural network performance\n",
        "neural_network_probabilities = final_neural_network.predict(X_test_normalized).flatten()\n",
        "\n",
        "fpr_neural, tpr_neural, classification_thresholds = roc_curve(y_test_set, neural_network_probabilities)\n",
        "optimal_threshold_index = np.argmax(tpr_neural - fpr_neural)\n",
        "optimal_classification_threshold = classification_thresholds[optimal_threshold_index]\n",
        "print(f\"\\nOptimal classification threshold: {optimal_classification_threshold:.3f}\")\n",
        "\n",
        "neural_network_predictions = (neural_network_probabilities > optimal_classification_threshold).astype(int)\n",
        "\n",
        "print(\"\\n--- NEURAL NETWORK MODEL PERFORMANCE ---\")\n",
        "print(classification_report(y_test_set, neural_network_predictions, target_names=['No Churn', 'Churn']))\n",
        "\n",
        "neural_network_confusion = confusion_matrix(y_test_set, neural_network_predictions)\n",
        "print(f\"Confusion matrix results:\\n{neural_network_confusion}\")\n",
        "\n",
        "neural_network_performance = {\n",
        "    'Accuracy Score': accuracy_score(y_test_set, neural_network_predictions),\n",
        "    'Precision Metric': precision_score(y_test_set, neural_network_predictions),\n",
        "    'Recall Metric': recall_score(y_test_set, neural_network_predictions),\n",
        "    'F1 Score': f1_score(y_test_set, neural_network_predictions),\n",
        "    'ROC-AUC Score': roc_auc_score(y_test_set, neural_network_probabilities)\n",
        "}\n",
        "\n",
        "print(\"\\nNeural network performance metrics:\")\n",
        "for metric_name, metric_value in neural_network_performance.items():\n",
        "    print(f\"  {metric_name}: {metric_value:.4f}\")\n",
        "\n",
        "# Training progress visualization\n",
        "fig, training_progress_plots = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "training_progress_plots[0].plot(complete_training_history.history['loss'], label='Training Loss')\n",
        "training_progress_plots[0].plot(complete_training_history.history['val_loss'], label='Validation Loss')\n",
        "training_progress_plots[0].set_title('Loss Function Progression', fontweight='bold')\n",
        "training_progress_plots[0].set_xlabel('Training Epoch')\n",
        "training_progress_plots[0].legend()\n",
        "\n",
        "training_progress_plots[1].plot(complete_training_history.history['accuracy'], label='Training Accuracy')\n",
        "training_progress_plots[1].plot(complete_training_history.history['val_accuracy'], label='Validation Accuracy')\n",
        "training_progress_plots[1].set_title('Accuracy Progression', fontweight='bold')\n",
        "training_progress_plots[1].set_xlabel('Training Epoch')\n",
        "training_progress_plots[1].legend()\n",
        "\n",
        "training_progress_plots[2].plot(complete_training_history.history['auc'], label='Training AUC')\n",
        "training_progress_plots[2].plot(complete_training_history.history['val_auc'], label='Validation AUC')\n",
        "training_progress_plots[2].set_title('AUC Metric Progression', fontweight='bold')\n",
        "training_progress_plots[2].set_xlabel('Training Epoch')\n",
        "training_progress_plots[2].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Neural network ROC curve visualization\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_neural, tpr_neural, color='darkorange', lw=2,\n",
        "         label=f'Neural Network ROC (AUC = {neural_network_performance[\"ROC-AUC Score\"]:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier Baseline')\n",
        "plt.scatter(fpr_neural[optimal_threshold_index], tpr_neural[optimal_threshold_index], color='red', s=100,\n",
        "            label=f'Optimal Threshold: {optimal_classification_threshold:.3f}')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate', fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontweight='bold')\n",
        "plt.title('Neural Network ROC Curve Analysis', fontweight='bold')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# =============================================================================\n",
        "# COMPREHENSIVE MODEL COMPARISON\n",
        "# =============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE MODEL PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Performance comparison table\n",
        "model_comparison_table = pd.DataFrame({\n",
        "    'Performance Metric': ['Accuracy Score', 'Precision Metric', 'Recall Metric', 'F1 Score', 'ROC-AUC Score'],\n",
        "    'Decision Tree Model': [tree_performance_metrics['Accuracy Score'], tree_performance_metrics['Precision Metric'],\n",
        "                      tree_performance_metrics['Recall Metric'], tree_performance_metrics['F1 Score'], tree_performance_metrics['ROC-AUC Score']],\n",
        "    'Neural Network Model': [neural_network_performance['Accuracy Score'], neural_network_performance['Precision Metric'],\n",
        "                       neural_network_performance['Recall Metric'], neural_network_performance['F1 Score'], neural_network_performance['ROC-AUC Score']]\n",
        "})\n",
        "\n",
        "print(\"\\n\" + model_comparison_table.to_string(index=False))\n",
        "\n",
        "# Visual model comparison\n",
        "fig, model_comparison_plots = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "metric_locations = np.arange(len(model_comparison_table['Performance Metric']))\n",
        "comparison_bar_width = 0.35\n",
        "\n",
        "tree_comparison_bars = model_comparison_plots[0].bar(metric_locations - comparison_bar_width/2, model_comparison_table['Decision Tree Model'], comparison_bar_width,\n",
        "                    label='Decision Tree', color='steelblue')\n",
        "network_comparison_bars = model_comparison_plots[0].bar(metric_locations + comparison_bar_width/2, model_comparison_table['Neural Network Model'], comparison_bar_width,\n",
        "                    label='Neural Network', color='coral')\n",
        "\n",
        "model_comparison_plots[0].set_xlabel('Performance Metric', fontweight='bold')\n",
        "model_comparison_plots[0].set_ylabel('Metric Value', fontweight='bold')\n",
        "model_comparison_plots[0].set_title('Model Performance Comparative Analysis', fontweight='bold')\n",
        "model_comparison_plots[0].set_xticks(metric_locations)\n",
        "model_comparison_plots[0].set_xticklabels(model_comparison_table['Performance Metric'])\n",
        "model_comparison_plots[0].legend()\n",
        "model_comparison_plots[0].set_ylim(0, 1)\n",
        "\n",
        "for bar_set in [tree_comparison_bars, network_comparison_bars]:\n",
        "    for individual_bar in bar_set:\n",
        "        bar_value = individual_bar.get_height()\n",
        "        model_comparison_plots[0].text(individual_bar.get_x() + individual_bar.get_width()/2., bar_value,\n",
        "                    f'{bar_value:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# Confusion matrix comparison visualization\n",
        "fig, confusion_comparison = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Decision tree confusion matrix\n",
        "sns.heatmap(tree_confusion_results, annot=True, fmt='d', cmap='Blues', ax=confusion_comparison[0],\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "confusion_comparison[0].set_title('Decision Tree Confusion Matrix', fontweight='bold')\n",
        "confusion_comparison[0].set_ylabel('Actual Classification')\n",
        "confusion_comparison[0].set_xlabel('Predicted Classification')\n",
        "\n",
        "# Neural network confusion matrix\n",
        "sns.heatmap(neural_network_confusion, annot=True, fmt='d', cmap='Reds', ax=confusion_comparison[1],\n",
        "            xticklabels=['No Churn', 'Churn'],\n",
        "            yticklabels=['No Churn', 'Churn'])\n",
        "confusion_comparison[1].set_title('Neural Network Confusion Matrix', fontweight='bold')\n",
        "confusion_comparison[1].set_ylabel('Actual Classification')\n",
        "confusion_comparison[1].set_xlabel('Predicted Classification')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ROC curve comparative analysis\n",
        "fpr_decision_tree, tpr_decision_tree, _ = roc_curve(y_test_set, tree_probability_predictions)\n",
        "fpr_neural_network, tpr_neural_network, _ = roc_curve(y_test_set, neural_network_probabilities)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr_decision_tree, tpr_decision_tree, linewidth=3, label=f'Decision Tree (AUC={tree_performance_metrics[\"ROC-AUC Score\"]:.3f})')\n",
        "plt.plot(fpr_neural_network, tpr_neural_network, linewidth=3, label=f'Neural Network (AUC={neural_network_performance[\"ROC-AUC Score\"]:.3f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classification Baseline')\n",
        "plt.xlabel('False Positive Rate', fontweight='bold')\n",
        "plt.ylabel('True Positive Rate', fontweight='bold')\n",
        "plt.title('ROC Curve Comparative Analysis', fontweight='bold')\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final analysis summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL ANALYTICAL SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "superior_model = 'Neural Network' if neural_network_performance['ROC-AUC Score'] > tree_performance_metrics['ROC-AUC Score'] else 'Decision Tree'\n",
        "print(f\"\\nOptimal performing model: {superior_model}\")\n",
        "print(f\"\\nDecision Tree Model - ROC-AUC: {tree_performance_metrics['ROC-AUC Score']:.4f}\")\n",
        "print(f\"Neural Network Model - ROC-AUC: {neural_network_performance['ROC-AUC Score']:.4f}\")\n"
      ]
    }
  ]
}